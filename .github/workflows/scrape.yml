name: Sports Scraper Pipeline

on:
  # Run daily at 0:00 UTC (2:00 CEST summer / 1:00 CET winter = ~2:00 Polish time)
  schedule:
    - cron: '0 0 * * *'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      sport:
        description: 'Sport to scrape (all = all sports)'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - football
          - basketball
          - volleyball
          - handball
          - hockey
          - tennis
      send_email:
        description: 'Send email notification'
        required: false
        default: true
        type: boolean

env:
  PYTHONIOENCODING: utf-8

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        sport: [football, basketball, volleyball, handball, hockey, tennis]
      fail-fast: false
      max-parallel: 6
    
    services:
      # FlareSolverr for Cloudflare bypass
      flaresolverr:
        image: ghcr.io/flaresolverr/flaresolverr:latest
        ports:
          - 8191:8191
        env:
          LOG_LEVEL: info
          CAPTCHA_SOLVER: none
    
    steps:
      # 1. Checkout repository
      - name: Checkout code
        uses: actions/checkout@v4
      
      # 2. Setup Python
      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      # 3. Setup Node.js (for Puppeteer)
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      
      # 4. Install Chrome for Puppeteer and Selenium
      - name: Install Chrome
        run: |
          sudo apt-get update
          sudo apt-get install -y chromium-browser chromium-chromedriver
          # Create symlink for compatibility
          sudo ln -sf /usr/bin/chromium-browser /usr/bin/google-chrome
          # Set environment variables
          echo "CHROME_BIN=/usr/bin/chromium-browser" >> $GITHUB_ENV
          echo "PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser" >> $GITHUB_ENV
      
      # 5. Install Python dependencies
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # üî• Cloudflare Bypass packages (curl_cffi is PRIMARY bypass method)
          sudo apt-get update && sudo apt-get install -y libcurl4-openssl-dev libssl-dev
          pip install curl-cffi cloudscraper httpx[http2]
          python -c "from curl_cffi import requests; print('curl_cffi OK')"
      
      # 6. Install Node.js dependencies (Puppeteer)
      - name: Install Node.js dependencies
        run: npm ci || npm install
      
      # 7. Wait for FlareSolverr to be ready
      - name: Wait for FlareSolverr
        run: |
          echo "‚è≥ Waiting for FlareSolverr to be ready..."
          FLARESOLVERR_READY=false
          for i in {1..30}; do
            if curl -s http://localhost:8191/health | grep -q '"status"'; then
              echo "‚úÖ FlareSolverr health endpoint responding!"
              TEST_RESPONSE=$(curl -s -X POST http://localhost:8191/v1 \
                -H "Content-Type: application/json" \
                -d '{"cmd":"request.get","url":"https://www.google.com","maxTimeout":10000}' \
                --max-time 15 2>/dev/null || echo "TIMEOUT")
              if echo "$TEST_RESPONSE" | grep -q '"status":"ok"'; then
                echo "‚úÖ FlareSolverr verified - working correctly!"
                FLARESOLVERR_READY=true
                break
              fi
            fi
            echo "Waiting... ($i/30)"
            sleep 2
          done
          if [ "$FLARESOLVERR_READY" != "true" ]; then
            echo "‚ö†Ô∏è FlareSolverr may not be fully ready, continuing anyway..."
          fi
      
      # 8. Run the scraper for this sport
      - name: Run scraper - ${{ matrix.sport }}
        env:
          FLARESOLVERR_URL: "http://localhost:8191/v1"
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
        run: |
          echo "üèÉ Scraping ${{ matrix.sport }}..."
          TODAY=$(date +%Y-%m-%d)
          python scrape_and_notify.py \
            --date $TODAY \
            --sports ${{ matrix.sport }} \
            --to "${{ secrets.EMAIL_RECIPIENT }}" \
            --from-email "${{ secrets.EMAIL_SENDER }}" \
            --password "${{ secrets.EMAIL_PASSWORD }}" \
            --provider gmail \
            --use-forebet \
            --use-sofascore \
            --use-odds \
            --use-gemini \
            --headless
      
      # 9. Upload results as artifact
      - name: Upload results - ${{ matrix.sport }}
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: results-${{ matrix.sport }}-${{ github.run_id }}
          path: |
            *.json
            *.html
            outputs/*.csv
            logs/
          retention-days: 7

